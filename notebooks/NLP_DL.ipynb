{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21666e07-419f-46c8-b306-2eaaaba16f1d",
   "metadata": {},
   "source": [
    "# NLP for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51488ffd-ed4a-48cf-a1b7-98627364a1f0",
   "metadata": {},
   "source": [
    "**Author**: Jonathan TRICARD\n",
    "\n",
    "**Summary**: using a dataset propose by sklearn, we build a DNN model to predict in which category of topic the text belong. Then, we try to explain the choice of the model.\n",
    "\n",
    "**ExplainDL**: create a file for each observation sected the given path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd451696-2696-4257-a350-31ce7f1a6574",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103464c-89c6-464f-b80a-0b8315e0fe42",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from readml.logger import ROOT_DIR\n",
    "from readml.explainers.dl.explain_dl import ExplainDL\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590eb0ef-4b98-4458-a92e-da3749319833",
   "metadata": {},
   "source": [
    "## Initialize the directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae5101-c77a-475c-ae25-dc7fa99b579b",
   "metadata": {},
   "source": [
    "We need need to build the path to save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d067e-c6c8-484a-85e9-7a3578ec92b1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_directories_dl(out_path, dir_to_create):\n",
    "    os.chdir(ROOT_DIR)\n",
    "    new_root = os.getcwd()\n",
    "    new_root = \"/\".join(new_root.split(\"/\")[:-1])\n",
    "    os.chdir(new_root)\n",
    "    start = out_path.index(\"/\") + 1\n",
    "    split = out_path[start:].split(\"/\")\n",
    "    for elt in split:\n",
    "        if not os.path.isdir(elt):\n",
    "            os.makedirs(elt)\n",
    "            os.chdir(elt)\n",
    "        else:\n",
    "            os.chdir(elt)\n",
    "    os.chdir(ROOT_DIR)\n",
    "\n",
    "    for elt in dir_to_create:\n",
    "        if not os.path.isdir(os.path.join(out_path, elt)):\n",
    "            os.makedirs(os.path.join(out_path, elt))\n",
    "            \n",
    "def create_dir_test():\n",
    "    dir_to_create = [\"text\"]\n",
    "    out_path = \"../outputs/notebooks/dl\"\n",
    "    initialize_directories_dl(out_path, dir_to_create)\n",
    "\n",
    "create_dir_test()\n",
    "output_path_text_dir = os.path.join(ROOT_DIR, \"../outputs/notebooks/dl\", \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c9f82",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088ddb5-5d0d-4f20-bcb5-5e9d6844472d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def create_text_data():\n",
    "    categories = [\n",
    "        'talk.religion.misc',\n",
    "        'sci.space',\n",
    "    ]\n",
    "    remove = ('headers', 'footers', 'quotes')\n",
    "    data_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "    data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42,remove=remove)\n",
    "    vectorizer = CountVectorizer(max_features = 20000, stop_words='english')\n",
    "    vectorizer.fit(data_train['data'])\n",
    "    X_train = pd.DataFrame(vectorizer.transform(data_train['data']).todense(), columns=vectorizer.get_feature_names())\n",
    "    X_test = pd.DataFrame(vectorizer.transform(data_test['data']).todense(), columns=vectorizer.get_feature_names())\n",
    "    y_train, y_test = data_train['target'], data_test['target']\n",
    "    df_train = pd.concat([X_train, pd.Series(y_train, name=\"target_col\")], axis = 1)\n",
    "    df_test = pd.concat([X_test, pd.Series(y_test, name=\"target_col\")], axis = 1)\n",
    "    return X_train, X_test, y_train, y_test, df_train, df_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cc665-c7cb-41bf-8d0f-8bc8bc467d6c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, df_train, df_test, vectorizer = create_text_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15360e8",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae8ee45-4c9f-4260-a6cb-b3af613ddc30",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def build_model_dnn_text(shape, n_classes, dropout=0.5):\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    n_layers = 4 # number of  hidden layer\n",
    "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0, n_layers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2180253-a8f1-49c9-b9d0-a065a4edc166",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = build_model_dnn_text(X_train.shape[1], n_classes = len(set(y_train)))\n",
    "model.fit(X_train, y_train) \n",
    "#lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65860e44",
   "metadata": {},
   "source": [
    "## Make intelligibility with readml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ecb6a-d052-4132-b67a-27c56b5ac20e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = model\n",
    "out_path = output_path_text_dir\n",
    "test_data = df_test\n",
    "target_col = \"target_col\"\n",
    "word2idx = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe4b9d-61df-4a2e-bf72-fc56209ce92b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "exp = ExplainDL(\n",
    "        model = model,\n",
    "        out_path = out_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793818d9-7610-44e4-ae9a-3d9415c6df91",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_test.target_col.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57f771-e336-4bae-9f11-7037c36e5c86",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "exp.explain_text(\n",
    "    test_data = df_test.head(5),\n",
    "    target_col = target_col,\n",
    "    word2idx = word2idx,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
