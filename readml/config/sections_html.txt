#COMMUN
L'interprétabilité globale consiste à comprendre et expliquer l'apprentissage du modèle. Cette étape est essentielle car elle permet de débugger le modèle, détecter les biais, faire confiance aux décisons du modèle une fois en production...

Les "partial dependency plots" (PDP), les "ICE Plots" et les "ALE Plots" sont un moyen de quantifier l'impact d'une (ou deux) features sur un modèle de machine learning. Ils permettent en effet de tracer, feature par feature, l'influence d'une variation de cette feature sur le modèle.
Dans le cas de features corrélées, il est conseillé d'utiliser les ALE plots au lieu des PDP.
Les plots SHAP (explication globale) permetent de connaître la feature importance mais aussi comment les valeurs des features ont impacté l'apprentissage du modèle. Il est préférable d'avoir un modèle de bonne qualité avant de lancer ces plots. En effet, il n'y a pas grand intérêt à expliquer un modèle peu fiable.
Ces plots peuvent être tracés indifféremment à partir de données d’entraînement, de test ou même de données non labellisées; la seule condition étant que ces données soit représentatives de la distribution normale des données.

#PDP
Les plots de dépendance partielle (PDP) montrent l'effet marginal qu’a une ou deux features sur les prévisions d’un modèle de machine learning. Un PDP est la moyenne des lignes d'un tracé ICE. 
Dans le cas de non-corrélation des features, l’interprétation est simple : les PD plots montrent comment la prédiction moyenne change quand la n-ième feature change. On fait varier une feature et on visualise les changements qu’entraînent cette variation sur les prédictions. Nous mesurons donc une relation de causalité entre la feature et la prédiction.
En pratique, il est conseillé d'utiliser les PDP et les ICE plots ensemble pour une meilleure interprétation.

#ICE
Un tracé ICE (Individual Conditional Expectation) visualise la dépendance de la prédiction sur une feature pour chaque observation séparément, ce qui donne une courbe par instance, contrairement à une courbe globale dans les tracés de dépendance partielle. Un PDP est la moyenne des lignes d'un tracé ICE.
Les courbes ICE sont plus intuitives et plus faciles à interpréter que les PDP. Chaque courbe représente les prédictions pour une valeur donnée si on varie la feature en question.
En pratique, il est conseillé d'utiliser les PDP et les ICE plots ensemble pour une meilleure interprétation.

#ALE
Les ALE (Accumulated Local Effects) décrivent en moyenne - sur la base de la distribution conditionnelle des features - l’influence des features sur la prédiction d’un modèle de Machine Learning.
Cette méthode fournit des explications globales agnostiques au modèle pour les modèles de classification et de régression sur des données tabulaires. Les ALE plots sont une alternative plus rapide et moins biaisée des plots PDP.

Si on prend l'exemple de prédiction des prix de maisons, pour estimer l’effet d’une superficie égale à 30 m², la méthode ALE va prendre en compte toutes les maisons qui ont une superficie d’environ 30 m² et calculer la différence entre les prédictions du prix des maisons qui ont une superficie de 31 m² et les prédictions du prix des maisons qui ont une superficie de 29 m². Ce qui nous donne l’effet pur de la feature superficie et ne mélange pas cet effet avec l’effet des autres features qui y sont corrélées. L’utilisation de la différence entre les prédictions élimine l’effet des autres features et rend la méthode plus robuste au corrélation des features.

#SHAP

Les valeurs SHAP permettent de montrer comment les valeurs de chaque feature contribuent (positivement ou négativement) à la variable cible (target). SHAP permet de tracer deux plots de features importance le "Summary bar plot" et le "Summary bee-swarm plot"

Le "Summary bar plot" liste les features les plus significatives/impactantes par ordre décroissant.​ Pour chaque feature, on calcule la moyenne absolue des SHAP values pour chaque feature, ce qui permet de comparer l'impact des features sur la sortie du modèle.

Le "Summary bee-swarm plot" se base sur les SHAP values pour montrer la distribution des impacts qu'a chaque feature sur la sortie du modèle. Ce plot regroupe plusieurs informations:​
- La Feature importance: les features sont classés par ordre décroissant selon leur impact sur le modèle (la somme des SHAP values sur tout le dataset).​
- L'impact: L'axe horizontal montre si l'effet d'une valeur est positif ou négatif sur la prédiction, il représente pour chaque feature la somme des SHAP values sur tout le dataset.​
- Les valeurs de chaque feature: les couleurs montrent si les valeurs de la feature sont élevées (bleu) ou faibles (rouge).​
